{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized correlation count 2D (NCC 2D) Workflow\n",
    "#### Author: Rodrigo dos Santos Maia CorrÃªa\n",
    "\n",
    "#### The University of Texas at Austin, Austin, Texas USA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow Goal\n",
    "\n",
    "Calculates Correlation sum, Correlation count and Normalized correlation count in two-dimensions for a point dataset within the area of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow Steps \n",
    "\n",
    "1. **Data Loading** - basic data loading, checking and visualization\n",
    "2. **Data Analysis** - Calculation of distances, azimuth, correlation sum and correlation count\n",
    "3. **Randomizations** - Randomizes the dataset within the analyzed area\n",
    "3. **Results** - Analysis of resulting calculations with graphs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part of the code was developed in colaboration with PGE Student Mahmood Shakiba\n",
    "\n",
    "### 2D Correlation Sum in a cicular domain of radius R at an array of lenght scales r\n",
    "def CorrSum_Random_2D_Array (r,R):\n",
    "    CorrSum_rndm_array = np.zeros(len(r))\n",
    "    for indx, ri in enumerate(r):        \n",
    "        CorrSum_rndm_array[indx] = (np.pi*ri**2)/(np.pi*np.max(r)**2)\n",
    "    return CorrSum_rndm_array\n",
    "\n",
    "### 2D Correlation Count in a cicular domain of radius R at an array of lenght scales r\n",
    "def CorrCount_Random_2D_Array (CorrSum):\n",
    "    if W>0:\n",
    "        CorrCount = CorrSum[(W*2):] - CorrSum[0:-(W*2)]\n",
    "    else:\n",
    "        CorrCount = CorrSum[1:] - CorrSum[0:-1]\n",
    "    return CorrCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading and Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                        # ndarrys for gridded data\n",
    "import pandas as pd                       # DataFrames for tabular data\n",
    "import os                                 # set working directory, run executables\n",
    "import matplotlib.pyplot as plt           # for plotting\n",
    "from scipy import stats                   # summary statistics\n",
    "import datetime\n",
    "import math\n",
    "import scipy.integrate as integrate              #for use of integrals\n",
    "from scipy.spatial.distance import pdist         #spatial statistics package\n",
    "from scipy.spatial.distance import squareform    #spatial statistics package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"\")             # set the working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define general settings for the workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nspl=100 # <------------Define number of length scales to be analysed\n",
    "W=2 # <-------------------Define Windowing (similar to smoothing)\n",
    "N_montecarlo=100 # <------------------- Define the number of montecarlo randomizations\n",
    "grad_scale='log' # <---------------Define the type of graduation for length scale ('lin' for linear or 'log' for logarithmic)\n",
    "Load_trace_map='yes' # <--------------Define if the trace map will be loaded for visualization (Use 'yes' to load fracture trace map)\n",
    "Weight='yes' #<-----------------Define if weighted normalized correlation count is calculated (Use 'yes' or 'no')\n",
    "Wf=5 #<-----------------Define the weighting factor (exagerates the weighting by the power of the factor)\n",
    "Azimuthal='yes' #<-----------------Define if Azimuthal analysis is calculated (Use 'yes' or 'no')\n",
    "nazi=24  ### <------------------------------SET NUMBER OF AZIMUTHS (suggested is 24 azimuths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the circular analysis area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define circular area of interest (radius and center coordinates)\n",
    "cr=  #Circle radius\n",
    "ccx=   #Circle center x-position\n",
    "ccy= #Circle center y-position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loads fracture barycenter point set with length and azimuth information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('', sep=',')# <-----------------Define fracture barycentre data file\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms azimuth information to pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pole=np.zeros((len(df['AZ'])))\n",
    "for i in range(0,len(df['AZ'])):\n",
    "    if df['AZ'][i]>=0 and df['AZ'][i]<=90:\n",
    "        pole[i]=df['AZ'][i]+90\n",
    "    if df['AZ'][i]>90 and df['AZ'][i]<=180:\n",
    "        pole[i]=df['AZ'][i]-90\n",
    "    if df['AZ'][i]>180 and df['AZ'][i]<=270:\n",
    "        pole[i]=df['AZ'][i]-90\n",
    "    if df['AZ'][i]>270 and df['AZ'][i]<=360:\n",
    "        pole[i]=df['AZ'][i]-270"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creates a table with fracture trace end points coordinates, ID and size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create fracture traces end positions\n",
    "Xt1=(df['X']+np.sin(np.radians(df['AZ']))*df['L']/2)\n",
    "Xt2=(df['X']-np.sin(np.radians(df['AZ']))*df['L']/2)\n",
    "Yt1=(df['Y']+np.cos(np.radians(df['AZ']))*df['L']/2)\n",
    "Yt2=(df['Y']-np.cos(np.radians(df['AZ']))*df['L']/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Organize fracture trace end positions with their respective ID and number of points (SZ)\n",
    "Xt=np.asarray(list(sum(zip(Xt1, Xt2), ())))\n",
    "Yt=np.asarray(list(sum(zip(Yt1, Yt2), ())))\n",
    "IDt=np.asarray(list(sum(zip(df['ID'], df['ID']), ())))\n",
    "Sz=np.linspace(2,2,(df['L'].size)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms all trace information in a table\n",
    "Random_traces=np.vstack((np.vstack((np.stack((Xt,Yt)),IDt)),Sz))\n",
    "dx=pd.DataFrame(Random_traces.T, columns=['X', 'Y', 'ID', 'SZ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots visualization maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot fracture traces and barycenters\n",
    "#Barycenters out of the circular analysis area MUST BE DELETED.\n",
    "plt.figure(figsize=(20,20))\n",
    "if Load_trace_map==('yes'):\n",
    "    for i in range(1, len(dx.ID)):\n",
    "        if (i-1)==0:\n",
    "                plt.plot(dx.X[i-1:i+int(dx.SZ[i-1]-1)], dx.Y[i-1:i+int(dx.SZ[i-1]-1)], 'r-')\n",
    "        if (i-1)-(dx.SZ[i-1])<0:\n",
    "                plt.plot(dx.X[i-int(dx.SZ[i-1]):i-1], dx.Y[i-int(dx.SZ[i-1]):i-1], 'r-')\n",
    "        if dx.ID[i]!=dx.ID[i-1]:\n",
    "            plt.plot(dx.X[i:i+int(dx.SZ[i])], dx.Y[i:i+int(dx.SZ[i])], 'r-')\n",
    "        \n",
    "plt.plot(df.X, df.Y, 'bo', label='Fracture barycenter', markersize=10)\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-', alpha=0.2)\n",
    "plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "circ=plt.Circle((ccx,ccy), cr, color='k', fill=False, linewidth=5)\n",
    "ax.add_artist(circ)\n",
    "#plt.minorticks_on()\n",
    "ax.set_xlim((ccx-cr),(ccx+cr))\n",
    "ax.set_ylim((ccy-cr),(ccy+cr))\n",
    "plt.xlabel('Position X(m)',size=18)\n",
    "plt.ylabel('Position Y(m)',size=18)\n",
    "plt.legend(loc='upper right', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates some of the working variables\n",
    "X=df['X']\n",
    "Y=df['Y']\n",
    "ID=df['ID']\n",
    "nmax=len(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates distances between barycenters:\n",
    "arr = np.stack((X, Y), axis=1)\n",
    "dist=squareform(pdist(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creates length scale values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'log' in grad_scale:\n",
    "    lscale=np.logspace(np.log10(np.min(dist[np.nonzero(dist)])),np.log10(cr*2),nspl) #logarithmic length scales #Builds the length scales that will be plotted and serve as reference to the correlation count and sum\n",
    "    print('Logarithmic scale')\n",
    "if grad_scale==('lin'):\n",
    "    lscale=np.linspace((np.min(dist[np.nonzero(dist)])),cr*2,nspl) #linear length scales\n",
    "    print('Linear scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creates the translational edge correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Formula obtained from appendix B in the book Statistical Analysis and modelling of Spatial Point Patterns -  Illian (2008)\n",
    "v=np.zeros((nmax,nmax))\n",
    "v=1/((2*(cr**2)*np.arccos(dist/(2*cr)))-((dist/2)*(4*(cr**2)-(dist**2))**(1/2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creates angle and length weigthing matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Weight==('yes'):\n",
    "    anw=np.zeros((nmax,nmax)) #Matrix holding the angle weights; Each element is filled below as an element-wise length x length x |cosine| multiplication\n",
    "    lenw=np.zeros((nmax,nmax)) #Matrix holding the length weights; Each element is filled below by an element-wise length x length multiplication\n",
    "    \n",
    "    k=0\n",
    "    j=0\n",
    "    i=0\n",
    "    for i in range(0,nmax):\n",
    "        for j in range(0,nmax):\n",
    "            if ID[i]!=ID[j]:\n",
    "                anw[i,j]=(np.minimum((np.abs(pole[i]-pole[j])),(180-np.abs(pole[i]-pole[j]))))**Wf\n",
    "                lenw[i,j]=(((df['L'][i])+(df['L'][j]))/2)**Wf\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculates Correlation sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_sum=np.zeros((np.size(lscale)))\n",
    "count=0\n",
    "if Weight==('yes'):\n",
    "    corr_suma=np.zeros((np.size(lscale)))\n",
    "    counta=0\n",
    "    corr_suml=np.zeros((np.size(lscale)))\n",
    "    countl=0\n",
    "    \n",
    "k=0\n",
    "i=0\n",
    "j=0\n",
    "while k<np.size(lscale): #Counts how many spacings exists below a certain length scale value, that is corr_sum\n",
    "    count=(np.sum(v*[np.where((dist < lscale[k]) & (dist != 0), 1, 0)]))\n",
    "    corr_sum[k]=count #saves the corrsum value in the position k for each analyzed length scale\n",
    "    count=0\n",
    "    if Weight==('yes'):\n",
    "        counta=np.sum(v*anw*[np.where((dist < lscale[k]) & (dist != 0), 1, 0)])\n",
    "        corr_suma[k]=counta #saves the corrsum value in the position k for each analyzed length scale\n",
    "        counta=0\n",
    "        \n",
    "        countl=np.sum(v*lenw*[np.where((dist < lscale[k]) & (dist != 0), 1, 0)])\n",
    "        corr_suml[k]=countl #saves the corrsum value in the position k for each analyzed length scale\n",
    "        countl=0\n",
    "        \n",
    "        \n",
    "    k=k+1\n",
    "k=0\n",
    "corr_sum=((corr_sum)/(np.sum(v)))\n",
    "if Weight==('yes'):\n",
    "    corr_suma=((corr_suma)/(np.sum(v*anw))) #Divided by summation of anwmin to normalized between 0 and 1\n",
    "    corr_suml=((corr_suml)/(np.sum(v*lenw))) #Divided by summation of lenwmin to normalize between 0 and 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculates Slope of Correlation sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_csum=np.zeros((np.size(corr_sum)-1))\n",
    "\n",
    "if W>0:\n",
    "    slope_csum=(np.log(corr_sum[(W*2):])-np.log(corr_sum[0:-(W*2)]))/(np.log(lscale[(W*2):])-np.log(lscale[0:-(W*2)]))\n",
    "else:\n",
    "    slope_csum=(np.log(corr_sum[1:])-np.log(corr_sum[0:-1]))/(np.log(lscale[1:])-np.log(lscale[0:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculates azimuthal correlation analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sets azimuthal tolerance and azimuthal directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Azimuthal==('yes'):\n",
    "    azitol=360/nazi/2\n",
    "    azidir=np.zeros(nazi)\n",
    "    for i in range(0,(nazi)):\n",
    "        azidir[i]=(360/nazi)*(i+1)-(360/nazi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Azimuthal==('yes'):\n",
    "    #Build azimuth matrix\n",
    "    azi=np.zeros((nmax,nmax))\n",
    "    for i in range(0,nmax):\n",
    "        for j in range(0,nmax):\n",
    "            if X[j]>=X[i] and Y[j]>=Y[i]:\n",
    "                azi[i,j]=np.degrees(np.arctan((X[j]-X[i])/(Y[j]-Y[i])))\n",
    "            if X[j]>=X[i] and Y[j]<Y[i]:\n",
    "                azi[i,j]=np.degrees(np.arctan((Y[i]-Y[j])/(X[j]-X[i])))+90\n",
    "            if X[j]<X[i] and Y[j]<Y[i]:\n",
    "                azi[i,j]=np.degrees(np.arctan((X[i]-X[j])/(Y[i]-Y[j])))+180\n",
    "            if X[j]<X[i] and Y[j]>=Y[i]:\n",
    "                azi[i,j]=np.degrees(np.arctan((Y[j]-Y[i])/(X[i]-X[j])))+270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Azimuthal==('yes'):\n",
    "    distaz=np.zeros((nazi,nmax,nmax))\n",
    "\n",
    "    for k in range (0,nazi):\n",
    "        mask=np.zeros((nmax,nmax))\n",
    "        mask=np.where(((azi<(azidir[k]+azitol)) & (azi>(azidir[k]-azitol)))|((azi>(azidir[k]-azitol+360)) & (azi<360)),1,0) \n",
    "        distaz[k,:,:]=dist*mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Azimuthal==('yes'):\n",
    "    corr_sumaz=np.zeros((nazi,np.size(lscale)))\n",
    "    count=0\n",
    "    k=0\n",
    "    i=0\n",
    "    for i in range(0,nazi):\n",
    "        while k<np.size(lscale): #Counts how many spacings exists below a certain length scale value, that is corr_sum\n",
    "            count=(np.count_nonzero(np.where(distaz[i] < lscale[k], distaz[i], 0)))\n",
    "            corr_sumaz[i,k]=count #saves the corrsum value in the position k for each analyzed length scale\n",
    "            count=0\n",
    "            k=k+1\n",
    "        k=0\n",
    "        i=i+1\n",
    "    corr_sumaz=((corr_sumaz)/(np.max(corr_sumaz)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Correlation count with windowing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if W>0:\n",
    "    corr_count=corr_sum[(W*2):]-corr_sum[0:-(W*2)]\n",
    "    if Weight==('yes'):\n",
    "        corr_counta=corr_suma[(W*2):]-corr_suma[0:-(W*2)]\n",
    "        corr_countl=corr_suml[(W*2):]-corr_suml[0:-(W*2)]\n",
    "                \n",
    "    if Azimuthal==('yes'):\n",
    "            corr_countaz=corr_sumaz[:,(W*2):]-corr_sumaz[:,0:(-W*2)]\n",
    "else:\n",
    "    corr_count=corr_sum[1:]-corr_sum[0:-1]\n",
    "    if Weight==('yes'):\n",
    "        corr_counta=corr_suma[1:]-corr_suma[0:-1]\n",
    "        corr_countl=corr_suml[1:]-corr_suml[0:-1]\n",
    "                \n",
    "    if Azimuthal==('yes'):\n",
    "            corr_countaz=corr_sumaz[:,1:]-corr_sumaz[:,0:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Randomizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculates monte carlo randomizations using homogeneus poisson point process: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=0\n",
    "\n",
    "all_corr_sum_rand=np.zeros(((np.size(corr_sum)),N_montecarlo))\n",
    "all_corr_count_rand=np.zeros(((np.size(corr_count)),N_montecarlo))\n",
    "            \n",
    "if Azimuthal==('yes'):  \n",
    "    all_corr_sum_randaz=np.zeros((nazi,(np.size(corr_sumaz[0])),N_montecarlo))\n",
    "    all_corr_count_randaz=np.zeros((nazi,(np.size(corr_countaz[0])),N_montecarlo))\n",
    "\n",
    "while r<N_montecarlo: #Builds the random position of the points and distance matrix for the randomizations\n",
    "#Simulates random points in the circular domain area\n",
    "    pirand=np.random.rand(nmax,1)*2*(math.pi)\n",
    "    radiusrand=np.sqrt(np.random.rand(nmax,1))*cr\n",
    "       \n",
    "    Xr=np.where((pirand>=0) & (pirand<((math.pi)/2)), ((ccx)+((np.sin(pirand))*radiusrand)), np.where((pirand>=((math.pi)/2)) & (pirand<(math.pi)), ((ccx)+((np.cos(pirand-((math.pi)/2)))*radiusrand)), np.where((pirand>=(math.pi)) & (pirand<((math.pi)*(3/2))), ((ccx)-((np.sin(pirand-(math.pi)))*radiusrand)), np.where((pirand>=((math.pi)*(3/2))) & (pirand<=(2*(math.pi))), ((ccx)-((np.cos(pirand-((math.pi)*(3/2))))*radiusrand)), 0))))\n",
    "    Yr=np.where((pirand>=0) & (pirand<((math.pi)/2)), ((ccy)+((np.cos(pirand))*radiusrand)), np.where((pirand>=((math.pi)/2)) & (pirand<(math.pi)), ((ccy)-((np.sin(pirand-((math.pi)/2)))*radiusrand)), np.where((pirand>=(math.pi)) & (pirand<((math.pi)*(3/2))), ((ccy)-((np.cos(pirand-(math.pi)))*radiusrand)),np.where((pirand>=((math.pi)*(3/2))) & (pirand<=(2*(math.pi))), ((ccy)+((np.sin(pirand-((math.pi)*(3/2))))*radiusrand)), 0))))\n",
    "    Xrand=Xr[:,0]\n",
    "    Yrand=Yr[:,0]\n",
    "    \n",
    "#Build the distance matrix for randomizations \n",
    "    arr_rand = np.stack((Xrand, Yrand), axis=1)\n",
    "    dist_rand=squareform(pdist(arr_rand))\n",
    "    \n",
    "#Build the translational edge correction\n",
    "    vr=np.zeros((nmax,nmax))\n",
    "    vr=1/((2*(cr**2)*(np.arccos(dist_rand/(2*cr))))-((dist_rand/2)*(4*(cr**2)-(dist_rand**2))**(1/2)))                            \n",
    "\n",
    "    #Build the corrsum matrix for randomizations \n",
    "    #Unweighted correlation sum\n",
    "    corr_sum_rand=np.zeros(((np.size(lscale))))\n",
    "    count=0\n",
    "    if Weight==('yes'):\n",
    "        #Angle weighted correlation sum\n",
    "        corr_sum_randa=np.zeros(((np.size(lscale))))\n",
    "        counta=0\n",
    "        #Lenght weighted correlation sum\n",
    "        corr_sum_randl=np.zeros(((np.size(lscale))))\n",
    "        countl=0\n",
    "       \n",
    "    k=0\n",
    "    i=0\n",
    "    j=0\n",
    "    while k<np.size(lscale): #random values - Counts how many spacings exists below a certain length scale value, that is corr_sum\n",
    "        count=(np.sum(vr*[np.where((dist_rand < lscale[k]) & (dist_rand != 0), 1, 0)]))\n",
    "        corr_sum_rand[k]=count #random values - saves the corrsum value in the position k for each analyzed length scale\n",
    "        count=0\n",
    "        if Weight==('yes'):\n",
    "            counta=np.sum(vr*anw*[np.where((dist_rand < lscale[k]) & (dist_rand != 0), 1, 0)])\n",
    "            corr_sum_randa[k]=counta #random values - saves the corrsum value in the position k for each analyzed length scale\n",
    "            counta=0\n",
    "            \n",
    "            countl=np.sum(vr*lenw*[np.where((dist_rand < lscale[k]) & (dist_rand != 0), 1, 0)])\n",
    "            corr_sum_randl[k]=countl #random values - saves the corrsum value in the position k for each analyzed length scale\n",
    "            countl=0\n",
    "            \n",
    "           \n",
    "            \n",
    "        k=k+1\n",
    "    k=0       \n",
    "    corr_sum_rand=((corr_sum_rand)/(np.sum(vr)))\n",
    "    if Weight==('yes'):\n",
    "        corr_sum_randa=((corr_sum_randa)/(np.sum(vr*anw)))\n",
    "        corr_sum_randl=((corr_sum_randl)/(np.sum(vr*lenw)))\n",
    "             \n",
    "     \n",
    "    if Azimuthal==('yes'):\n",
    "    #Build Azimuthal matrix for randomizations \n",
    "        azi_rand=np.zeros((nmax,nmax))\n",
    "        i=0\n",
    "        j=0\n",
    "        for i in range(0,nmax):\n",
    "            for j in range(0,nmax):\n",
    "                if Xrand[j]>=Xrand[i] and Yrand[j]>=Yrand[i]:\n",
    "                    azi_rand[i,j]=np.degrees(np.arctan((Xrand[j]-Xrand[i])/(Yrand[j]-Yrand[i])))\n",
    "                if Xrand[j]>=Xrand[i] and Yrand[j]<Yrand[i]:\n",
    "                    azi_rand[i,j]=np.degrees(np.arctan((Yrand[i]-Yrand[j])/(Xrand[j]-Xrand[i])))+90\n",
    "                if Xrand[j]<Xrand[i] and Yrand[j]<Yrand[i]:\n",
    "                    azi_rand[i,j]=np.degrees(np.arctan((Xrand[i]-Xrand[j])/(Yrand[i]-Yrand[j])))+180\n",
    "                if Xrand[j]<Xrand[i] and Yrand[j]>=Yrand[i]:\n",
    "                    azi_rand[i,j]=np.degrees(np.arctan((Yrand[j]-Yrand[i])/(Xrand[i]-Xrand[j])))+270\n",
    "\n",
    "        #Build distance matrix per azimuth\n",
    "        dist_randaz=np.zeros((nazi,nmax,nmax))\n",
    "        k=0\n",
    "        for k in range (0,nazi):\n",
    "            mask_rand=np.zeros((nmax,nmax))\n",
    "            mask_rand=np.where(((azi_rand<(azidir[k]+azitol)) & (azi_rand>(azidir[k]-azitol)))|((azi_rand>(azidir[k]-azitol+360)) & (azi_rand<360)),1,0) \n",
    "            dist_randaz[k,:,:]=dist_rand*mask_rand\n",
    "\n",
    "        #Build correlation sum azimuthal\n",
    "        corr_sum_randaz=np.zeros((nazi,np.size(lscale)))\n",
    "        count=0\n",
    "        k=0\n",
    "        i=0\n",
    "        for i in range(0,nazi):\n",
    "            while k<np.size(lscale): #Counts how many spacings exists below a certain length scale value, that is corr_sum\n",
    "                countaz=(np.count_nonzero(np.where(dist_randaz[i] <= lscale[k], dist_randaz[i], 0)))\n",
    "                corr_sum_randaz[i,k]=countaz #saves the corrsum value in the position k for each analyzed length scale\n",
    "                countaz=0\n",
    "                k=k+1\n",
    "            k=0\n",
    "            i=i+1\n",
    "        corr_sum_randaz=((corr_sum_randaz)/(np.max(corr_sum_randaz)))\n",
    "\n",
    "#Build the corrcount matrix for randomizations        \n",
    "    if W>0:\n",
    "        corr_count_rand=corr_sum_rand[(W*2):]-corr_sum_rand[0:(-W*2)]\n",
    "        if Weight==('yes'):\n",
    "            corr_count_randa=corr_sum_randa[(W*2):]-corr_sum_randa[0:(-W*2)]\n",
    "            corr_count_randl=corr_sum_randl[(W*2):]-corr_sum_randl[0:(-W*2)]\n",
    "                       \n",
    "        if Azimuthal==('yes'):\n",
    "            corr_count_randaz=corr_sum_randaz[:,(W*2):]-corr_sum_randaz[:,0:(-W*2)]\n",
    "    else:\n",
    "        corr_count_rand=corr_sum_rand[1:]-corr_sum_rand[0:-1]\n",
    "        if Weight==('yes'):\n",
    "            corr_count_randa=corr_sum_randa[1:]-corr_sum_randa[0:-1]\n",
    "            corr_count_randl=corr_sum_randl[1:]-corr_sum_randl[0:-1]\n",
    "                        \n",
    "        if Azimuthal==('yes'):\n",
    "            corr_count_randaz=corr_sum_randaz[:,1:]-corr_sum_randaz[:,0:-1]\n",
    "            \n",
    "                                           \n",
    "#Builds the matrix to store the results of all realizations                                                \n",
    "    all_corr_sum_rand[:,r:r+1]=np.reshape(corr_sum_rand[:],(np.size(corr_sum),1))\n",
    "    all_corr_count_rand[:,r:r+1]=np.reshape(corr_count_rand[:],(np.size(corr_count),1))\n",
    "                   \n",
    "    if Azimuthal==('yes'):\n",
    "        all_corr_sum_randaz[:,:,r]=corr_sum_randaz\n",
    "        all_corr_count_randaz[:,:,r]=corr_count_randaz\n",
    "    \n",
    "    \n",
    "    Rper=str(int(((r+1)/N_montecarlo)*100)) + str('%') + str('Randomization Nr.')+ str(r+1)\n",
    "    print(Rper, end='\\r')\n",
    "    \n",
    "    r=r+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculates statistical results from randomizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates the P2.5, P50 and P97.5 of the randomizations of unweighted case\n",
    "P50_ccount=np.percentile(all_corr_count_rand,50, axis=1)\n",
    "P50_csum=np.percentile(all_corr_sum_rand,50, axis=1)\n",
    "P97_ccount=np.percentile(all_corr_count_rand,97.5, axis=1)\n",
    "P2_5_ccount=np.percentile(all_corr_count_rand,2.5, axis=1)   \n",
    "       \n",
    "if Azimuthal==('yes'):\n",
    "    #Calculates the P2.5, P50 and P95 of the randomizations of azimuthal case\n",
    "    P50_ccountaz=np.percentile(all_corr_count_randaz,50, axis=2)\n",
    "    P97_ccountaz=np.percentile(all_corr_count_randaz,97.5, axis=2)\n",
    "    P2_5_ccountaz=np.percentile(all_corr_count_randaz,2.5, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculates randomizations with analytical solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part of the code was developed in colaboration with PGE Student Mahmood Shakiba\n",
    "R = cr;                             # domain radius\n",
    "dmin = lscale[0];                   # minimum length scale\n",
    "dmax = lscale[nspl-1];              # maximum length scale\n",
    "dstep = nspl;                       # number of length scales\n",
    "\n",
    "\n",
    "d = lscale     # array of length scales\n",
    "\n",
    "CorrSum = CorrSum_Random_2D_Array (d,R)\n",
    "CorrCount = CorrCount_Random_2D_Array(CorrSum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculates normalized correlation count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates the normalized correlation count for the unweighted case\n",
    "ncorr_count=corr_count/CorrCount\n",
    "P97_nccount=P97_ccount/CorrCount\n",
    "P2_5_nccount=P2_5_ccount/CorrCount\n",
    "P50_nccount=P50_ccount/CorrCount\n",
    "\n",
    "if Weight==('yes'):\n",
    "    #Creates the normalized correlation count for the angle weighted case\n",
    "    ncorr_counta=corr_counta/CorrCount\n",
    "    \n",
    "    #Creates the normalized correlation count for the length weighted case\n",
    "    ncorr_countl=corr_countl/CorrCount   \n",
    "       \n",
    "if Azimuthal==('yes'):\n",
    "    #Creates the normalized correlation count for the angle weighted case\n",
    "    ncorr_countaz=corr_countaz/P50_ccountaz\n",
    "    ncorr_countaz97=corr_countaz/P97_ccountaz\n",
    "    ncorr_countaz2_5=corr_countaz/P2_5_ccountaz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compares montecarlo and analytical unweighted solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots Correlation sum\n",
    "plt.subplot(121)\n",
    "plt.plot(d,CorrSum,color = 'blue', label='Analytical')\n",
    "plt.plot(lscale,P50_csum,label='Montecarlo', color = 'green')\n",
    "plt.xlabel('Length scale (m)',size=18)\n",
    "plt.ylabel('Correlation Sum',size=18)\n",
    "plt.xticks(fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "\n",
    "if grad_scale=='log':\n",
    "    plt.yscale(\"log\"); plt.xscale(\"log\");\n",
    "\n",
    "plt.legend(loc='lower right',fontsize=15)\n",
    "plt.grid(True,'both')\n",
    "\n",
    "#Plots Correlation count\n",
    "plt.subplot(122)\n",
    "if W>1:\n",
    "    plt.plot(d[1+(W-1):-W],CorrCount,color = 'blue', label='Analytical')\n",
    "    plt.plot(lscale[1+(W-1):-W],P50_ccount,label='Montecarlo', color = 'green')\n",
    "elif W==1:\n",
    "    plt.plot(d[1:-W],CorrCount,color = 'blue', label='Analytical')\n",
    "    plt.plot(lscale[1:-W],P50_ccount,label='Montecarlo', color = 'green')\n",
    "else:\n",
    "    plt.plot(d[1:],CorrCount,color = 'blue', label='Analytical')\n",
    "    plt.plot(lscale[1:],P50_ccount,label='Montecarlo', color = 'green')\n",
    "\n",
    "plt.xlabel('Length scale (m)',size=18)\n",
    "plt.ylabel('Correlation Count',size=18)\n",
    "plt.xticks(fontsize = 15)\n",
    "plt.yticks(fontsize = 15)\n",
    "\n",
    "if grad_scale=='log':\n",
    "    plt.yscale(\"log\"); plt.xscale(\"log\");\n",
    "\n",
    "plt.legend(loc='lower right',fontsize=15)\n",
    "plt.grid(True,'both')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.4, hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots Correlation sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7.5))\n",
    "plt.plot(lscale,corr_sum,label='Unweighted', linewidth=2, color='k')\n",
    "if Weight==('yes'):\n",
    "    plt.plot(lscale,corr_suma,label='Angle weighted', linewidth=2, color='g')\n",
    "    plt.plot(lscale,corr_suml,label='Length weighted', linewidth=2, color='r')\n",
    "    \n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "plt.minorticks_on()\n",
    "plt.xlabel('Length scale (m)',size=18)\n",
    "plt.ylabel('Correlation Sum',size=18)\n",
    "plt.legend(fontsize=15)\n",
    " \n",
    "if grad_scale=='log':\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "plt.show\n",
    "plt.savefig(fname='Csum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(15,7.5))\n",
    "\n",
    "ax1.set_xlabel('Length scale (m)',size=18)\n",
    "ax1.set_ylabel('Correlation Sum',size=18)\n",
    "ax1.plot(lscale,corr_sum,label='Unweighted CSum', linewidth=2, color='k')\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "ax1.set_ylim(None,1)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "ax2.set_ylabel('Slope',size=18)  # we already handled the x-label with ax1\n",
    "\n",
    "if W>1:\n",
    "    ax2.plot(lscale[1+(W-1):-W],slope_csum,label='Slope', linewidth=2, color='b')\n",
    "\n",
    "elif W==1:\n",
    "    ax2.plot(lscale[1:-W],slope_csum,label='Slope', linewidth=2, color='b')\n",
    "    \n",
    "else:\n",
    "    ax2.plot(lscale[1:],slope_csum,label='Slope', linewidth=2, color='b')\n",
    "    \n",
    "\n",
    "ax2.set_ylim(0,3)\n",
    "\n",
    "plt.legend(fontsize=15)\n",
    "plt.minorticks_on() \n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()\n",
    "plt.savefig(fname='slope_csum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots Normalized Correlation count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7.5))\n",
    "\n",
    "if W>1:\n",
    "    plt.plot(lscale[1+(W-1):-W],ncorr_count,label='Unweighted', color='k')\n",
    "    plt.plot(lscale[1+(W-1):-W],P97_nccount,label='Confidence limits (95%)', color='k',linestyle='--')\n",
    "    plt.plot(lscale[1+(W-1):-W],P2_5_nccount, color='k',linestyle='--')\n",
    "    \n",
    "elif W==1:\n",
    "    plt.plot(lscale[1:-W],ncorr_count,label='Unweighted', color='k')\n",
    "    plt.plot(lscale[1:-W],P97_nccount,label='Confidence limits (95%)', color='k',linestyle='--')\n",
    "    plt.plot(lscale[1:-W],P2_5_nccount,color='k',linestyle='--')\n",
    "    \n",
    "else:\n",
    "    plt.plot(lscale[1:],ncorr_count,label='Unweighted', color='k')\n",
    "    plt.plot(lscale[1:],P97_nccount,label='Confidence limits (95%)', color='k',linestyle='--')\n",
    "    plt.plot(lscale[1:],P2_5_nccount,color='k',linestyle='--')\n",
    "    \n",
    "\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "plt.minorticks_on()\n",
    "plt.xlabel('Length scale (m)',size=18)\n",
    "plt.ylabel('Normalized Correlation Count',size=18)\n",
    "plt.legend(fontsize=15)\n",
    "\n",
    "if grad_scale=='log':\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "plt.show\n",
    "plt.savefig(fname='NCCU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Weight==('yes'):\n",
    "    plt.figure(figsize=(15,7.5))\n",
    "\n",
    "    if W>1:\n",
    "        plt.plot(lscale[1+(W-1):-W],ncorr_count,label='Unweighted', linewidth=1, color='k')\n",
    "        plt.plot(lscale[1+(W-1):-W],ncorr_counta,label='Angle weighted', linewidth=3, color='g')\n",
    "        plt.plot(lscale[1+(W-1):-W],P97_nccount,label='Confidence limits (95%)', color='k',linestyle='--')\n",
    "        plt.plot(lscale[1+(W-1):-W],P2_5_nccount, color='k',linestyle='--')\n",
    "        \n",
    "    elif W==1:\n",
    "        plt.plot(lscale[1:-W],ncorr_count,label='Unweighted', linewidth=1, color='k')\n",
    "        plt.plot(lscale[1:-W],ncorr_counta,label='Angle weighted', linewidth=3, color='g')\n",
    "        plt.plot(lscale[1:-W],P97_nccount,label='Confidence limits (95%)', color='k',linestyle='--')\n",
    "        plt.plot(lscale[1:-W],P2_5_nccount,color='k',linestyle='--')\n",
    "        \n",
    "    else:\n",
    "        plt.plot(lscale[1:],ncorr_count,label='Unweighted', linewidth=1, color='k')\n",
    "        plt.plot(lscale[1:],ncorr_counta,label='Angle weighted', linewidth=3, color='g')\n",
    "        plt.plot(lscale[1:],P97_nccount,label='Confidence limits (95%)', color='k',linestyle='--')\n",
    "        plt.plot(lscale[1:],P2_5_nccount,color='k',linestyle='--')\n",
    "       \n",
    "    plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "    plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "    plt.minorticks_on()\n",
    "    plt.xlabel('Length scale (m)',size=18)\n",
    "    plt.ylabel('Weighted Normalized Correlation Count',size=18)\n",
    "    plt.legend(fontsize=15)\n",
    "\n",
    "    if grad_scale=='log':\n",
    "        plt.xscale(\"log\")\n",
    "        plt.yscale(\"log\")\n",
    "\n",
    "    plt.show\n",
    "    plt.savefig(fname='NCCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if Weight==('yes'):\n",
    "    plt.figure(figsize=(15,7.5))\n",
    "\n",
    "    if W>1:\n",
    "        plt.plot(lscale[1+(W-1):-W],ncorr_count,label='Unweighted', linewidth=1, color='k')\n",
    "        plt.plot(lscale[1+(W-1):-W],ncorr_countl,label='Length weighted', linewidth=3, color='r')\n",
    "        plt.plot(lscale[1+(W-1):-W],P97_nccount,label='Confidence limits (95%)', color='k',linestyle='--')\n",
    "        plt.plot(lscale[1+(W-1):-W],P2_5_nccount, color='k',linestyle='--')\n",
    "        \n",
    "    elif W==1:\n",
    "        plt.plot(lscale[1:-W],ncorr_count,label='Unweighted', linewidth=1, color='k')\n",
    "        plt.plot(lscale[1:-W],ncorr_countl,label='Length weighted', linewidth=3, color='r')\n",
    "        plt.plot(lscale[1:-W],P97_nccount,label='Confidence limits (95%)', color='k',linestyle='--')\n",
    "        plt.plot(lscale[1:-W],P2_5_nccount,color='k',linestyle='--')\n",
    "                     \n",
    "    else:\n",
    "        plt.plot(lscale[1:],ncorr_count,label='Unweighted', linewidth=1, color='k')\n",
    "        plt.plot(lscale[1:],ncorr_countl,label='Length weighted', linewidth=3, color='r')\n",
    "        plt.plot(lscale[1:],P97_nccount,label='Confidence limits (95%)', color='k',linestyle='--')\n",
    "        plt.plot(lscale[1:],P2_5_nccount,color='k',linestyle='--')\n",
    "        \n",
    "    plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "    plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "    plt.minorticks_on()\n",
    "    plt.xlabel('Length scale (m)',size=18)\n",
    "    plt.ylabel('Weighted Normalized Correlation Count',size=18)\n",
    "    plt.legend(fontsize=15)\n",
    "\n",
    "    if grad_scale=='log':\n",
    "        plt.xscale(\"log\")\n",
    "        plt.yscale(\"log\")\n",
    "\n",
    "    plt.show\n",
    "    plt.savefig(fname='NCCL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if Weight==('yes'):\n",
    "    plt.figure(figsize=(15,7.5))\n",
    "\n",
    "    if W>1:\n",
    "        plt.plot(lscale[1+(W-1):-W],ncorr_count,label='Unweighted', linewidth=1, color='k')\n",
    "        plt.plot(lscale[1+(W-1):-W],ncorr_counta,label='Angle weighted', linewidth=2, color='g')\n",
    "        plt.plot(lscale[1+(W-1):-W],ncorr_countl,label='Length weighted', linewidth=2, color='r')\n",
    "                 \n",
    "\n",
    "    elif W==1:\n",
    "        plt.plot(lscale[1:-W],ncorr_count,label='Unweighted', linewidth=1, color='k')\n",
    "        plt.plot(lscale[1:-W],ncorr_counta,label='Angle weighted', linewidth=2, color='g')\n",
    "        plt.plot(lscale[1:-W],ncorr_countl,label='Length weighted', linewidth=2, color='r')\n",
    "                \n",
    "    else:\n",
    "        plt.plot(lscale[1:],ncorr_count,label='Unweighted', linewidth=1, color='k')\n",
    "        plt.plot(lscale[1:],ncorr_counta,label='Angle weighted', linewidth=2, color='g')\n",
    "        plt.plot(lscale[1:],ncorr_countl,label='Length weighted', linewidth=2, color='r')\n",
    "                \n",
    "    plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "    plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n",
    "    plt.minorticks_on()\n",
    "    plt.xlabel('Length scale (m)',size=18)\n",
    "    plt.ylabel('Normalized Correlation Count',size=18)\n",
    "    plt.legend(fontsize=15)\n",
    "\n",
    "    if grad_scale=='log':\n",
    "        plt.xscale(\"log\")\n",
    "        plt.yscale(\"log\")\n",
    "\n",
    "    plt.show\n",
    "    plt.savefig(fname='NCCquad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots Azimuthal Correlation Sum in Polar plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Azimuthal==('yes'):\n",
    "    aziplot=np.append(azidir,360)\n",
    "    corrsum_plot= np.vstack((corr_sumaz, corr_sumaz[0,:]))\n",
    "    l, theta = np.meshgrid(lscale, np.radians(aziplot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#BUILD THE CORRELATION SUM AZIMUTHAL PLOT / POLAR PLOT\n",
    "if Azimuthal==('yes'):\n",
    "    vmin=0\n",
    "    vmax=1\n",
    "    levelscf = np.linspace(vmin, vmax, 100)\n",
    "    fig1, ax = plt.subplots(subplot_kw=dict(projection='polar'),figsize=(30,30))\n",
    "    ax.set_theta_zero_location(\"N\")\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    ax.set_title('2D correlation Sum', fontsize=40)\n",
    "    contourf=ax.contourf(theta, l, corrsum_plot,cmap='coolwarm',levels=levelscf,vmin=vmin, vmax=vmax)\n",
    "    cbar = plt.colorbar(contourf)\n",
    "    cbar.ax.tick_params(labelsize=30)\n",
    "    ax.tick_params(labelsize=30)\n",
    "    ax.grid(color='k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Azimuthal==('yes'):\n",
    "    ncorrcount_plot= np.vstack((ncorr_countaz, ncorr_countaz[0,:]))\n",
    "    ncorrcount97_plot= np.vstack((ncorr_countaz97, ncorr_countaz97[0,:]))\n",
    "    ncorrcount2_5_plot= np.vstack((ncorr_countaz2_5, ncorr_countaz2_5[0,:]))\n",
    "\n",
    "    if W>1:\n",
    "        l, theta = np.meshgrid(lscale[1+(W-1):-W], np.radians(aziplot))\n",
    "    elif W==1:\n",
    "        l, theta = np.meshgrid(lscale[1:-W], np.radians(aziplot))\n",
    "    else:\n",
    "        l, theta = np.meshgrid(lscale[1:], np.radians(aziplot))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#BUILD THE NORMALIZED CORRELATION COUNT AZIMUTHAL PLOT / POLAR PLOT\n",
    "if Azimuthal==('yes'):\n",
    "    vmin=0\n",
    "    vmax=2\n",
    "    levelscf = np.linspace(vmin, vmax, 100)\n",
    "    levelsc = -9999,1,9999\n",
    "    fig1, ax = plt.subplots(subplot_kw=dict(projection='polar'),figsize=(30,30))\n",
    "    ax.set_theta_zero_location(\"N\")\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_title('2D Normalized Correlation Count', fontsize=40)\n",
    "    contourf=ax.contourf(theta, l, ncorrcount_plot,cmap='coolwarm',levels=levelscf,vmin=vmin, vmax=vmax, extend='both')\n",
    "    contour=ax.contour(theta, l, ncorrcount97_plot,levels=levelsc, colors='r',linewidth=5)\n",
    "    contourb=ax.contour(theta, l, ncorrcount2_5_plot,levels=levelsc, colors='b',linewidth=5)\n",
    "    cbar = plt.colorbar(contourf)\n",
    "    cbar.ax.tick_params(labelsize=30)\n",
    "    ax.tick_params(labelsize=30)\n",
    "    ax.grid(color='k',linewidth=2)\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
